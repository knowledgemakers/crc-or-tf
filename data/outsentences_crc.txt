
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
		
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>

		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
		
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>

		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
		
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
		
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
	
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17</head><p>Touch, E-textiles and Participation -Using e-textiles to facilitate hands-on making workshops with visually-impaired users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emilie Giles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry Patrizia Paci</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
BACKGROUND</head><p>Distributed real-time process control systems are notoriously difficult to develop.
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Individual component's abstractions can be combined to define a complete cross system specification.</p><p>The cross system functional model can be used to accurately predict cross system performance prior to the detail design of each component.
Whilst it is impractical to manually calculate this performance, it can be simulated using a computer generated animator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>The need for modelling performance for the early detection of performance failures is well established.
<ref type="bibr">[1]</ref>.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
<ref type="bibr">[2]</ref>.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
<ref type="bibr">[3]</ref>.</p><p>Fifteen performance approaches identified to combat the 'fix- it-later' approach have been summarised <ref type="bibr">[4]</ref>.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
Each of these performance methods can contribute to the performance analysis of Distributed Real- time Process Control Systems but rely on system architecture and software design being wholly or partly complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>As a solution to the above difficulties we propose modelling individual system elements, sensors, actuators, displays and communication systems as periodic processes associated with a statistical description of the errors and delays.
Existing performance methods based on MARTE <ref type="bibr">[5]</ref> using the techniques described above can be used for individual components to calculate performance.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These early models can be used to specify the performance requirements of individual system elements or if individual system components contain an iterative processing architecture can be used to develop processing element software.</p><p>System architects establish the components and component communications early in the system lifecycle.
Tools based on SysML 1.1 <ref type="bibr">[6]</ref> provide a method of specifying the system architecture.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
<ref type="bibr">[7]</ref>.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
These models can</p><p>This provides three opportunities to analyse the performance of the overall system.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
The third opportunity is to replace the functional chains generated from the PIM with implemented functional elements from the MDA Platform Specific Models (PSM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH PROGRESS</head><p>Three case studies have been undertaken to investigate suitable methodologies.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
A proof of concept has been demonstrated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>Further evaluation of the animator is required to establish the issues involved with scalability for use with more complex distributed systems involving more complex functional chains.</p><p>The transformation between functional models and UML MDA PIMs needs be investigated with the aim of developing automatic transformation techniques between these two models in both directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSION</head><p>The proposed methodology enables performance to be animated or calculated early in the design process generating models automatically focused on sections of the system which relate to individual performance end events prior to architectural and software structures being finalised.
These models can be used, in conjunction with equipment specifications and interface control documents to specify individual system element functional and non-functional requirements.</p><p>Where individual system elements are implemented using an iterative architecture, functional models can be used to develop system element processing software.
<ref type="bibr">[3]</ref> M. Petre, "UML in practice," presented at the Proceedings of the 2013 International Conference on Software Engineering, San Francisco, CA, USA, 2013.
<ref type="bibr">[4]</ref> S. Balsamo, A.
Di Marco, P. Inverardi, and M. Simeoni, "Model-based performance prediction in software development: a survey," Software Engineering, IEEE <ref type="bibr">Transactions on, vol.
30, pp.
295310, 2004</ref>.
<ref type="bibr">[5]</ref> OMG, "OMG Profile 'UML Profile for MARTE' 1.0," 2009.
<ref type="bibr">[6]</ref> OMG, "OMG Systems Modelling Language (SysML) 1.2," 2010.
<ref type="bibr">[7]</ref> P. C. Eeles, Peter, The process of Software Architecting: Addison Wesley Professional, 2009.
<ref type="bibr">[8]</ref> N. K. Sinha, "Control Systems," ed: New Age International, 1994.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.
INTRODUCTION</head><p>A System of Systems (SoS) is an arrangement of useful and independent complex systems, which are integrated into a bigger system that delivers unique capabilities <ref type="bibr">[1]</ref>.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that, while at the same time achieving key quality objectives such as performance, reliability or security, a SoS can offer more functionality than the sum of the constituent systems <ref type="bibr">[2]</ref>.
However, as the SoS is formed by the integration of independent complex systems, this will increase the complexity of the SoS to, at least, one more order than its component systems <ref type="bibr">[3]</ref>.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Due to voluminous requirements documents, constant changes into requirements, complexity and presence of different stakeholders, conflicts arise within each component system and also across the SoS due to unexpected interactions between components, users and SoS goals <ref type="bibr">[4]</ref>.</p><p>In the SoS environment, there are different component systems that often came from different domains, developed by different teams, under different circumstance and time.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
On the other hand, adaptation is way to support a system and help it to modify their behaviour and structure in response to their perception of the environment <ref type="bibr">[5]</ref>.
Moreover, the SoS is known to be adaptive by nature <ref type="bibr">[6]</ref>.
Thus, this research aims to propose a way to support the SoS in its task to manage conflicting requirements by adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
CONFLICTING REQUIREMENTS</head><p>Conflicting requirements isn't a new area of study.
Conflicting requirements are the requirements held by two or more stakeholders that provoke an inconsistency <ref type="bibr">[4]</ref>.
Also, <ref type="bibr">[7]</ref> define inconsistency as "any situation in which two parts of a specification do not obey some relationship that should hold between them".
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
In the SoS context, there are three main parts that may influence the appearance of conflicting requirements: each component system, the SoS and the users preferences.</p><p>Thus, as each component system exists into an independent context so they will present their own requirements and these requirements may be conflicting with some of the SoS global requirements, moreover all them could be conflicting with the user's preference as well.
In fact, conflicting requirements will exists and this can't be avoided, the way to handle with them is by managing the inconsistency that triggers them <ref type="bibr">[7]</ref>.
In this context, there are many examples of tackling the problem of conflicting requirements <ref type="bibr">[8]</ref> [9] <ref type="bibr">[10]</ref>.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
Thus, there are some examples of tackling the problem of managing conflicting requirements at runtime <ref type="bibr">[11]</ref> [12] <ref type="bibr">[13]</ref>  <ref type="bibr">[14]</ref>.
But, even considering conflicting requirements at runtime, all these approaches don't take the SoS environment in consideration.</p><p>There are many different and important issues to be taken into account and that is able to bring challenges to the process of manage them.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
Thus, by taking account the SoS environment, <ref type="bibr">[15]</ref> presents ReMinds, a flexible framework that is able to monitor events in the SoS at runtime.
Further, they applied ReMinds at an industrial SoS and presented a promising result when working with realistic event loads <ref type="bibr">[16]</ref>.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
Also, the existing approaches can't be directly applied without reasoning about this environment and its particularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
ADAPTATION IN THE SOS ENVIRONMENT</head><p>The main features of a SoS are autonomy, connectivity between the systems and emergent behaviours <ref type="bibr">[17]</ref>.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
In fact, adaptation is present and necessary to the SoS environment <ref type="bibr">[6]</ref>.
So, in other words, the SoS is adaptive by nature <ref type="bibr">[6]</ref>.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
Indeed, <ref type="bibr">[18]</ref> argue about the importance of the self-adaptive system be aware of its own requirements at runtime, in order to reason if they are being attended and, if necessary, manage conflicts between them.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
Also, <ref type="bibr">[19]</ref> uses adaptation to propose a component to manage some conflicts between Java classes that arise from integration and evolution.
Also, <ref type="bibr">[20]</ref> introduce MobiWeb and argue that it is able to manage conflicting requirements by using a priority scheme in the adaptation process.
Finally, <ref type="bibr">[21]</ref> proposes an extension to the KAOS method by incorporating "adaptive goals", in order to represent adaptation strategies to tackle conflicting goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
CONCLUSION AND RESEARCH PROPOSAL</head><p>In fact, Conflicting requirements exists in all kinds of systems and they are an important problem to manage.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>We are currently living in the evolutionary software development (ESD) era where software projects have short initial development and early start of evolution and maintenance, in which software evolves in order to fulfill maintenance tasks.
Around 50%-75% of the total effort is spent on software maintenance.</p><p>Concept location is the first step of applying software changes for software maintenance according to the phased model of software change (PMSC).
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
These studies often create a query interface for accepting users' input and displaying the returned relevant program elements.</p><p>The doctoral research targets at the former problem, which is named as change request (CR) localization, because I) The input has a richer textual description, II) The challenges are greater because some practical factors, such as the running overhead, should be taken into consideration and III) The result will have more practical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Change Request Localization Without Looking into Software History</head><p>Traditionally, researchers examine the software project either in a static or dynamic way to localize a concept.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Therefore, they are not suitable to address CR localization alone.</p><p>In contrast to traditional static or dynamic analysis based approaches, information retrieval (IR) techniques <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> have been applied to localize change requests.
Treating source codes as textual documents, classical IR models <ref type="bibr">[1]</ref> are able to localize natural language descriptions of change requests with a decent accuracy and a low computational complexity.
IR models can also be combined with static/dynamic analysis approaches <ref type="bibr">[4]</ref> to further enhance the performance.
However, these approaches are insufficient to recommend what files should be newly created, which is quite common for a change request.</p><p>Therefore, the doctoral research will be based on IR-based framework for addressing the CR localization problem.
The framework will be different from the existing models in that it should be able to recommend on newly added files in addition to modified files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Change Request Localization By Looking into Software History</head><p>The changes applied to source codes and non-source code artefacts in order to address change requests are recorded in software historical repositories.
Developers are interested in building automatic approaches to learn from these information in order to better localize the ongoing change requests.</p><p>So far, the majority of research is focused on identifying co-changing program elements in the software history, and thereby generating additional program elements based on the co-changing rules extracted <ref type="bibr">[2]</ref> .
However, these approaches are not reliable because even though two program elements co- change many times in history, it does not mean they should also co-change for the current change request.</p><p>Some other researchers are concerned about associating source code changes to non-source code artefacts, e.g.
matching the commits to bug reports, and then identifying relevant artefacts to the ongoing change request to support a better CR localization <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">6]</ref>.
However, they are based on simple heuristics <ref type="bibr">[5]</ref>, key words matching <ref type="bibr">[6]</ref> or lexical similarities <ref type="bibr">[3]</ref>.
The semantic coupling of changes from heterogeneous artefacts, unfortunately, still remains an untouched area.</p><p>Therefore, the doctoral research will establish the semantic coupling of heterogeneous changes in software historical repositories.
Any ongoing change request will be matched to its semantically similar sets of co-changed artefacts, and the program elements within the sets will be returned as semantically relevant candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PROPOSED SOLUTION</head><p>Based on the introduction of the related works, the doctoral research is motivated to adopt IR-based approaches and incorporate software history information for addressing CR localization.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
II) Can one effectively capture the semantic representation for the co-changing heterogeneous artefacts in the software history?</p><p>The proposed IR-based framework is composed of two modules: lexical matching and semantic matching.</p><p>Lexical matching analyzes the source code of the software system at request time and computes the lexical similarities between each program element and the input change request.
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The package structure of the software system will be extracted and combined with the classical IR models to compute the scores for both files and packages.</p><p>Semantic matching aggregates historical co-changed source codes and non-source code artefacts into change sets.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The semantically relevant program elements within the change sets will therefore be returned.</p><p>Both lexical matching and semantic matching are be able to generate the relevant program elements alone from a distinctive aspect.
Considering the complementary nature of the two approaches, a fusion of the them will be proposed to obtain more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
INITIAL STUDY</head><p>An exploration study has been conducted on the lexical matching module for CR localization.
The task is to map change requests to java class files.
In order to recommend on newly added files, we treat java packages as additional retrieval targets.</p><p>Since java packages and files have a hierarchical structure, the problem is anological to the structured IR task, which is a well-studied IR area with many off-the-shelf strategies.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
Finally, the packages and files are ranked together based on the computed scores.</p><p>The applied strategies are evaluated on datasets collected from Apache Software Foundation with Jira issue tracking system.
The performances are measured in three commonly used IR metrics, namely TopK, MRR and MAP, compared to two classical IR models, UM and vector space model (VSM).</p><p>Before the evaluation, we first computed the ratio of added files in the total number of touched files in the golden set, and found out that around 30%-50% of the touched files are newly added ones, which suggests that it is necessary to localize newly added files aside from modified files.</p><p>The evaluation results show that the performances of three structured IR strategies are consistently better than the baseline models, indicating that the package structure has been effectively incorporated through structured IR strategies.
Furthermore, Aggregation has the largest value in 9 over the 12 metrics for comparision (3 metrics on 4 datasets), so it is the best-performed lexical matching strategy for CR localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
FUTURE WORK</head><p>The two critical components of the proposed solution for CR localization are lexical matching and semantic matching.
The initial study provides a lexical matching framework without involving any historical information.
The future work in the doctoral research will be focused on the following aspects:</p><formula xml:id="formula_0">I)</formula><p>In order to facilitate the doctoral research, datasets from more software systems with different programming languages should be collected, in which the changes on both source codes and non-source code artefacts will be extracted.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
III) As the main research focus, the semantic coupling of heterogeneous changes among software artefacts in the historical repositories will be explored, and a semantic matching model will be built.</p><p>IV) The approach to combining the lexical matching model and the semantic matching model will be proposed, and a tool will be developed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The motivation for this work is that insights from studies of language and from psychological theories of reasoning can benefit how we design computer languages.
As an exemplar of this, the work investigates Description Logics (DLs) 1 , which are used extensively for knowledge representation and the design of which is grounded in formal logic.</p><p>The original syntax for DLs was based on notation from mathematical logic and there was an early realization that this represented a barrier for the many non-logicians who wished to use these languages and who came from disciplines such as biology, geography and business.
Consequently, the original syntax has largely been replaced by the Manchester OWL Syntax (MOS) <ref type="bibr">[1]</ref> which is aimed at making DLs more accessible.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
Yet, as the quote above from Grice <ref type="bibr">[2]</ref> maintains, natural language and formal logic diverge, even when they share the same vocabulary.
In effect, the keywords in MOS do not have precisely the same meaning as when used in English, where in any case the meaning depends on context.</p><p>There has been an awareness of this problem since the early days of MOS.
For example, Rector et al.
<ref type="bibr">[3]</ref> described the problems experienced by novices, based on early experience of teaching DLs.</p><p>The problems are not purely ones of understanding.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
<ref type="bibr">[4]</ref> investigated the difficulty of following reasoning steps in DL expressed in natural language.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
<ref type="bibr">[4]</ref>, as in MOS, deviates from the usage in everyday English.</p><p>Research into the difficulty of DLs has as yet failed to take into account insights from studies of language and from cognitive psychology.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The goal of the work described in this abstract is to build on this research to understand the difficulties experienced with DLs and to make recommendations to mitigate those difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THEORIES OF REASONING</head><p>Prominent in the debate about the nature of human reasoning are two opposing viewpoints: rule-based and model- based.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
see Rips <ref type="bibr">[5]</ref>.
The latter suggests that mental models are constructed to represent a given situation, e.g.
see Johnson-Laird <ref type="bibr">[6]</ref>.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
It has been suggested that the two approaches are present to varying degrees in everyday reasoning <ref type="bibr">[7]</ref>.
In any case, it is likely that even non-logicians using DLs will have been exposed to some training in logic and may make use of a rule-based as well as a model-based approach.</p><p>Another relevant and complementary theory is that of relational complexity (RC).
Here complexity is defined "as a function … of the number of variables that can be related in a single cognitive representation" <ref type="bibr">[8]</ref>.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The claim is that the accuracy of a chain of reasoning steps is dependent on the maximum RC of the individual steps.</p><p>The role-based and mode-based viewpoints have proved useful in understanding the varying approaches taken when reasoning about propositional logic and the existential and universal quantifiers in DLs.
Relational complexity, on the other hand, has provided a metric for comparing the complexity of relations used in DLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
STUDY 1 -COMMON PATTERNS &amp; CONSTRUCTS</head><p>The first study was based on commonly used DL patterns and constructs.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
<ref type="bibr">[9]</ref>, and the author's own usage survey <ref type="bibr">[10]</ref>.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Participants were required to indicate whether the inference was valid or non-valid.</p><p>The study identified a number of difficulties <ref type="bibr">[11]</ref>.
Two of these related particularly to difficulties with language and reasoning and were followed up in subsequent studies:</p><p>•!
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
This can be explained by the model- based theory and has been observed by other researchers investigating naïve reasoners <ref type="bibr">[12]</ref>.</p><p>•!
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
This was investigated further in the second study, see below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
STUDY 2 -CONTROLLED COMPARISONS</head><p>A follow-up study re-examined some of the issues considered in the first study and went beyond that study to investigate the interaction of negation and quantification and multiple quantification.
The study consisted of 34 questions organized into four sections.</p><p>A question involving functionality was answered less accurately than a question of the same relational complexity involving transitivity, suggesting that functionality is inherently harder than transitivity.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
Both cases illustrate a divergence between MOS and common English usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
STUDY 3 -MODIFYING THE LANGUAGE</head><p>The previous two studies suggested some changes to MOS which were then trialed in a third study.
Syntax changes which had a favourable effect on performance included:</p><p>•!
The use of the additional keyword, solely, to indicate the direction of uniqueness in a relation.</p><p>•!
The replacement of and with intersection aided the comprehension of negated conjunction.</p><p>•!
The use of except in place of and not aided comprehension; this is useful in cases where exceptions are being defined.</p><p>•!
The replacement of only by noneOrOnly and some by including helped clarify the precise meaning of these keywords.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
CONCLUSIONS</head><p>The studies described here have shown how theories of reasoning and language can help explain the problems experienced in understanding and reasoning with DLs, and can also be used to mitigate those difficulties.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>This talk presents the results of two experiments that proposes and validates a simple, rapid and quantitative method for assessing the ability of sonifications to convey the fine temporal detail of real time Electroencephalography (EEG) signals for neurofeedback purposes.</p><p>In neurofeedback, aspects of brain activity are monitored and presented back to the user, in real-time to help them learn to modify their own brain behavior <ref type="bibr">[1]</ref>.
Learning theory suggests that the more rapidly and accurately the feedback mirrors brain activity, the more efficient the learning <ref type="bibr">[2]</ref>.
Therefore the ability of a sonification to convey temporally complex EEG data rapidly and accurately is critical for neurofeedback.</p><p>Consequently a key aim of this research is to develop and prototype a simple method for the initial comparison, screening and selection of sonifications for real time EEG neurofeedback.
As one of the main difficulties for the field of neurofeedback in general and EEG sonification neurofeedback in particular, is the costly and time-consuming nature of the research studies that can typically require between 10 to 50 sessions of 30 to 40 min duration with 20 to 60 participants <ref type="bibr">[3]</ref>.
Therefore before embarking on 400 hours of data collection to test the efficacy of a new sonification it would be prudent to have a "quick and dirty" assessment method that could reveal the relative abilities of a particular sonification mapping and see if it merited a more rigorous study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
METHODS</head><p>As an initial step in the development of a methodology to compare the effectiveness of real-time EEG sonifications, two simple real-time capable sonifications were selected, Amplitude Modulation and Frequency Modulation, in order to establish baseline comparisons.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
Whereas with Frequency Modulation (FM) sonification the frequency of the sine wave increases or decreases.</p><p>Experiment 1 used an off-line listening method to compare two different sonification techniques.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Moreover, averaging or smoothing of the EEG data to slow it down and make it more track-able is likely to reduce the information content and degrade perception of the finely detailed signal, which of course, is the opposite objective of this research.</p><p>This experiment combines several measures -the tracking task, standardized workload questionnaire (NASA-TLX) <ref type="bibr">[4]</ref> and subjective ratings of arousal and valence of the sound.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESULTS</head><p>The main finding of Experiment 1 was that FM sonification led to higher tracking accuracy than AM-based sonification despite being rated as more mentally, physically and temporally demanding and taking more effort.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
Experiment 2 extended the evaluation of the same two sonifications into a real-life neurofeedback training environment where participants were engaged in an emotional regulation task by training down left frontal alpha in order to reduce negative emotions <ref type="bibr">[5]</ref>.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSION</head><p>Frequency Modulation sonification showed both a better tracking accuracy in experiment 1 and a greater improvement in the subjective ratings of emotions in experiment 2.
The second experiment showed similar workload findings from the first study.</p><p>These two studies demonstrate a "proof of concept", showing that the tracking task is simple, rapid and practical and can provide a quantitative assessment to compare the ability of sonifications to convey real-time EEG data for neurofeedback.</p><p>The next experiment will combine the methods used in the previous two experiments with a 2 channel EEG sonification.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Towards the Automatic Generation of Dynamic</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Music plays a large part in setting the mood and atmosphere in digital games by working together with the narrative, as well as complementing the gameplay.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
Players that tire of listening to the same piece of music being repeated over and over experience listener fatigue <ref type="bibr">[1]</ref>.
This goes against what is referred to as the golden rule of audio design -"never let the player become annoyed or bored by the repetitiveness of the sound" <ref type="bibr">[2]</ref>.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
In particular, I will be focusing on creating a generative music system that is capable of generating affective music that fits the game experience, ultimately increasing player immersion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>Procedural content generation (hereby referred to as PCG), defined by <ref type="bibr">[3]</ref> as the "use of a formal algorithm to generate game content (such as levels or game rules) that would typically be produced by a human", has been used by game developers since the 1980s.
Several possible uses for the use of PCG algorithms in games have been proposed by <ref type="bibr">[4]</ref>, such as speeding up the process of content creation, facilitating the exploration of new ideas for designers and developers, and saving memory on disk.
However, there are very few examples of algorithmic music generation in commercial games.
One example is Ballblazer <ref type="bibr">[5]</ref>, a 3D sports game that uses the "riffology" algorithm <ref type="bibr">[6]</ref> to choose from a list of riffs and change them slightly by removing notes, altering the volume, or adapting the tempo.
Another is Monkey Island 2: LeChuck's Revenge <ref type="bibr">[7]</ref>, a pirate themed point-and-click adventure game that attempts to solve the problem of transitioning between different pieces of in-game music by using horizontal re- sequencing -changing queued segments of music depending on in-game events, and vertical re-orchestration -adding or removing different musical elements or melody lines to the piece currently being played <ref type="bibr">[8]</ref>.
Other examples in research include <ref type="bibr">[9]</ref>, who use first-order Markov models to dynamically generate chords in real-time in a maze game depending on the amount of danger the player character is experiencing, and <ref type="bibr">[10]</ref>, who describe a system architecture that is similar in structure to a musical performance.</p><p>There is a vast amount of literature on different techniques and algorithms that have been used to generate music such as grammars, neural networks, cellular automata, evolutionary algorithms, expert systems, and Markov chains <ref type="bibr">[11,</ref><ref type="bibr">12,</ref><ref type="bibr">6]</ref>.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
In particular, I will be focusing on the use of a multiple viewpoint system, which consists of different viewpoints of a sequence that can be combined appropriately to give a final prediction <ref type="bibr">[13]</ref>.
These viewpoints are normally modeled using Markov chains.
To my knowledge, the use of this technique has not yet been attempted in an algorithmic music generator for games.</p><p>Emotions are described by <ref type="bibr">[14]</ref> as being relatively short in duration, intense, and somewhat volatile, and some work has been done to identify elements of music that contribute to induce particular emotions in people <ref type="bibr">[15]</ref>.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
Markov models were used by <ref type="bibr">[16]</ref> in order to generate harmony to increase or decrease tension in a maze game.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
A different approach by <ref type="bibr">[17]</ref> required the levels for a puzzle/maze game to be generated first by using genetic algorithms to follow a specific tension curve, and a suitable soundscape was then generated to fit that curve.
All aforementioned systems reported good results.</p><p>Immersion, defined by <ref type="bibr">[18]</ref> as "the sensation of being transported into a mediated alternate reality", is an essential part of the gameplay experience, and music plays a large part in this.
An experiment by <ref type="bibr">[19]</ref> shows that a point-and-click adventure game developed to be played equally well by both visually impaired and sighted peopled shows that an equal amount of immersion is experienced by both, indicating that the role that audio plays in creating an immersive experience is stronger than graphics.
Furthermore, the use of dynamic music seems to increase the level of immersion in players greatly compared to non-dynamic music <ref type="bibr">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RESEARCH APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Objectives</head><p>As stated, my research will focus on the automatic generation of dynamic music for computer games to increase player immersion.
In particular, I will be focusing on creating suitable music that is able to convey particular emotions by manipulating particular musical features as described in <ref type="bibr">[15]</ref>, and creating suitable transitions between different affective music.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
Several methods of evaluation have been identified by <ref type="bibr">[21]</ref>, which I will use and build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.!
Limitations</head><p>The system will be built based on Western music theory; this is due to the large amount of literature that is based on this theory.
Furthermore, the scope of the project will be restricted to one genre of computer games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>In conclusion, I have identified that the automatic generation of music for games is a gap that has only recently started picking up steam.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
<ref type="bibr">Piwek and Boyer [6]</ref> the question generation (QG) community has adopted the following definition for this specific branch of Artificial Intelligence:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deduction in Question Generation</head><p>"Question Generation is the task of automatically generating questions from various inputs such as raw text, database, or semantic representation.
Question Generation is regarded as a discourse task involving the following four steps: (1) when to ask a question, <ref type="formula">(2)</ref> what the question is about, i.e.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
The desired relation between input and output is usually given by requiring the generation of questions that answer the input.</p><p>According to OED <ref type="bibr">[4]</ref>, deduction is "the process of deduct- ing or drawing a conclusion from a principle already known or assumed".
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
If the deduction rules are sound they won't introduce any incorrect information and if input and knowledge are reliable we will obtain certain conclusions.</p><p>The literature <ref type="bibr">[5]</ref> raises the question on the role of repres- entation in increasing question precision for diverse question types.
The purpose of the research is to show that it is possible to give a mathematical characterisation of the input text representation and that such representation is linked to the kind of questions that can be generated.</p><p>The sentence "All phones are Android devices" lets us generate several questions.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
".</p><p>The immediate application of the research is in tutoring, where we don't only want to test whether the student is receiving factual information about a subject, but also whether she is making connections about it.
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
Deduction is part of this process.</p><p>Much work has been done on the way reasoning is repres- ented and performed on automatic systems, but establishing how the mind delineates a cognitive model is still more an art than science.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
My purpose is to provide instruments to facilitate the work of those who strive to produce an invention or obtain a discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
RELATED WORK</head><p>In this section I highlight some question classification criteria given in the literature since I think it is an important step in the effort of providing a link between Question type and the representation space.</p><p>Graesser, Person and Huber <ref type="bibr">[1]</ref> gives a taxonomy of 18 questions with informative purpose in a tutoring environment.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
For the moment I would focus on questions rather than on enquiries.</p><p>A broad subdivision of question categories is provided by Rus and Graesser <ref type="bibr">[8]</ref> according to whether they tap causal mechanisms.
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Shallow questions such as who, what, when, where don't tap causal mechanisms.</p><p>It is important to highlight the fact that so far there is no characterisation of systems based on the representation they support.
Yao, Bouma and Zhang <ref type="bibr">[10]</ref> illustrates the behaviour of a system based on semantic transformations.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
MrsQG did worst for which questions and best for who questions.</p><p>The variety of questions produced could be improved, probably the most challenging area for systems based on semantics.</p><p>III.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
I assume other knowledge in the system is represented in the same way the input sentence is, but, admittedly, in general, this assumption does not hold.</p><p>An excellent candidate for the study of representation is the family of Description Logic (DL).
This is a group of fragments of first order logic (FOL) whose hierarchy of expressive power and complexity is already quite structured <ref type="bibr">[11]</ref>.
However, the main advantage provided by the system is the existence of repositories of information encoded in such framework, the ontologies.</p><p>Computer science uses the term "ontology" to denote an engineering artifact, usually a model of the world, represented by a vocabulary specific to a domain, and also providing the intended meaning for that vocabulary <ref type="bibr">[2]</ref>.</p><p>Axioms in the language are subdivided in three categories: A-Box (assertion box or A), T-Box (terminology box or T ), R-Box (Role box or R), often subsumed by T .
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Being a brother is a kind of being a sibling).</p><p>Although any kind of entailment can be checked in a DL, the current systems support (1) Satisfiability, (2) Subsumption, (3) Equivalence, (4) Disjointness for classes, (5) Decision whether an individual is an instance of a class, (6) Consistency check- ing.
I have yet to investigate whether there are entailments that cannot be represented by these 6 classes.</p><p>A notable mention of another candidate logic system has to be given to the Natural Logic described by Valencia <ref type="bibr">[9]</ref>, written under the supervision of Johan van Benthem.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
MacCartney <ref type="bibr">[3]</ref> provides a computational model for Inferences with a Natural Logic system based on the one described by Sánchez Valencia, without the usage of a CG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.
RESULTS</head><p>I am working in understanding whether there are general ways of describing the formal part of the system.
The idea of formalising semantics is not new.
However, the risk involved is that the process could prove computationally unfeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.
CONCLUSIONS AND FURTHER WORK</head><p>Although there are ways of characterising the structure of representation, I have yet to prove that the characterisation can be related to the question type.
And I have yet to understand whether it is more a semantic or syntactical problem.</p><p>I have to focus on the generation aspect after having worked on the logical aspect.
Perhaps an empirical study can be performed that checks the quality of the deductive questions generated from a text, whether they are logically, grammatically, syntactically correct and they are categorised by question type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.
ACKNOWLEDGEMENT</head><p>I wish to thank my supervisors, Paul Piwek and Allan Third who are being patient, supportive and very generous with their time, providing useful and competent insights on the direction and material of the research.
</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material-Oriented Musical Interaction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
ABSTRACT</head><p>This paper lays out a particular approach to engaging with musical instruments and creative tools more generally, referred to here as material-oriented interaction.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
This approach is discussed in relation to free improvisation - an area of music that exemplifies the material-oriented perspective -and then related to related concepts in HCI such as Ludic Design, Reflective Design and Reflection-in-Action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>In his thesis on the use of technology in contemporary computer music, Worth <ref type="bibr">[17]</ref> draws a distinction between an idealist notion of artistic creation where technology is viewed as an ideally transparent medium for communicating ideas, and a more material-oriented approach which sees the technology as a necessary and creative mediation that can be a source of ideas itself rather than simply a means for their transmission.
This latter approach can be traced in other areas of contemporary music such as experimental instrumental composition in the tradition of John Cage <ref type="bibr">[8]</ref> and particularly free improvisation <ref type="bibr">[13]</ref>, <ref type="bibr">[9]</ref>.</p><p>The implications of a full consideration of material-oriented approaches to interacting with technology appear to contradict many of the established principles in human-computer interaction (HCI) design and research.
For example, the importance of clarity, simplicity and efficiency <ref type="bibr">[15]</ref>, <ref type="bibr">[7]</ref> appear much more closely suited to the idealist approach where the technology should be transparent and not present obstacles to the communication of existing ideas.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
For such an approach the materials need to be able "kick back" <ref type="bibr">[12]</ref> against the user; there must be the possibility of being surprised by the interaction.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
This paper examines this approach to interaction in specific musical contexts and examines its relevance for HCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
MATERIAL-ORIENTED INTERACTIONS IN FREE IMPROVISATION</head><p>Free improvisation provides a fascinating site for exploring the relationships between tools, musicians and musical creation, and for considering material-oriented interactions.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
It is not only a means to an end, it is a source of material, and technique for the improvisor is often an exploitation of the natural resources of the instrument" <ref type="bibr">[1]</ref>.
This attitude is common with free improvising musicians: that their relationship with instruments is bidirectional, with the instrument having a strong influence on the decisions taken by the musician to the extent that they are often ascribed their own agency <ref type="bibr">[4]</ref>, <ref type="bibr">[8]</ref>.
These perspectives highlight the importance of surprising, unexpected elements in creative interactions, and the importance of being able to explore.</p><p>Exploration is a key element in free improvisation; improvisers want to be able to search and discover new sounds and ways of interacting with their instruments <ref type="bibr">[5]</ref> and their "natural resources".
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
Prévost is keen to stress at his improvisation workshops that improvised performances are not mere presentations of things that have already been found, but a continuation of these active exploratory processes <ref type="bibr">[13]</ref>.</p><p>A risk for HCI is to assume that the searching process is an inefficiency to be streamlined or removed to facilitate creativity.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
McDermott et al <ref type="bibr">[10]</ref> suggest that if interaction designers had been present at the inception of many acoustic instruments, they would likely have dismissed them as overly complicated.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
Improvisers often exploit the unstable regions of the interaction, where unexpected things can occur <ref type="bibr">[8]</ref>.
In such instances, the complexity and instability of the interaction are inseparable from the "natural resources" of the instrument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MATERIAL-ORIENTED INTERACTIONS AND HCI</head><p>MOI can be related to several key concepts in existing HCI research.
Donald Schön's notion of reflection-in-action (RIA) <ref type="bibr">[14]</ref> shares some similarities in that it highlights the importance of unexpected results and of allowing "talk back" from the emerging situation to influence the direction of the work.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
Schön's examples of pencilled architectural drawings <ref type="bibr">[14]</ref>, musical bells <ref type="bibr">[2]</ref>, design software and musical improvisors <ref type="bibr">[3]</ref>, are not concerned with unexpected interactions with the tools, but by relating the emerging situation back to the original task.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
By contrast, MOI focuses on interaction designs that promote nuanced exploration of the interaction itself.</p><p>Ludic design as discussed by Gaver also provides a useful perspective that can be related to material-oriented interaction design, in that through taking play seriously it prizes interactions that foster exploration, surprise and improvisation <ref type="bibr">[6]</ref>.</p><p>Sengers et al bring both ludic design and reflection-in- action together into their notion of reflective design <ref type="bibr">[16]</ref>.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
They are focused on reflection as critical reflection however, and while there is common ground, we are focused on surprise in relation to creativity and creative interactions.</p><p>Thinking about material-oriented interactions in HCI provides a different focus for designers, opening up engagement with unpredictable and potentially difficult, complex interactions as sites for explorations rather than inhibitors in reaching a pre-defined goal.
This is explored in a recent paper by the present authors investigating the use of nonlinear dynamics in musical interactions <ref type="bibr">[11]</ref> V.!
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
We demonstrate their relevance in specific musical areas, but anticipate exploring the influence of such attitudes in many other creative situations, musical and otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>The feel of a fabric is a sensory experience that we examine through touch.
As discussed by Philippe et al.
<ref type="bibr">[12]</ref> consumers have been increasingly judging their purchases through the 'softness' of touch, along with the 'quality'.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
as 'Exploratory Procedures' or 'EPs' <ref type="bibr">[8]</ref>.
For example, to discover the texture of a fabric, one might rub one's hand gently over the surface.</p><p>Touch is important in communication, particularly for users who are non-verbal or for those with no sight.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
<ref type="bibr">[9]</ref>.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
<ref type="bibr">[11]</ref> have explored this, taking a hand- crafted approach to making 'soft' sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
RELATED WORK</head><p>As a technology, e-textiles are used more often to achieve visual outcomes rather than tactile ones.
Garments and objects are designed for looking at rather than touching, such as the dresses designed by fashion companies who have embraced this technology <ref type="bibr">[4]</ref>.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
<ref type="bibr">[16]</ref>, who have explored the potential of haptic wearable devices for users with mental health issues, or the work by <ref type="bibr">Schelle et al.
[14]</ref>, who have created a pillow to be used for interpersonal contact between dementia patients and their family members or carers.
These projects embrace user centered design approaches to engage with the user, but what about the user themselves learning to work with these technologies?</p><p>With the development of physical computing prototyping boards, particularly the Lilypad Arduino by Leah Buechley, research in STEM education has become an ever-growing area.
There is much research around encouraging more young people, particularly girls, to gain an interest in engineering <ref type="bibr">[2]</ref>, and there are outreach-based research projects with 'at-risk' communities, such as the research by <ref type="bibr">Kuznetsov et al.
[7]</ref> that focuses on education and empowerment.
Rode et al.
<ref type="bibr">[13]</ref> have also used e-textiles in their research, finding that there is a need to include an 'A' for arts in STEM (STEAM).
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
Participatory Design is an approach which can help to address this question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
PARTICIPATORY DESIGN</head><p>Gregory quotes Bratteteig et al.
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Ideally, users at many levels participate so that change can be shaped from several perspectives' <ref type="bibr">[5:63]</ref>.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
<ref type="bibr">[10]</ref> worked with visually impaired sound engineers to develop non-visual prototypes, both low-fi prototypes and highly-malleable digital ones, to work toward designing interfaces which use audio, tactile and haptic displays.
The final outcome of this project is a device called the 'Haptic Wave', which allows for the mapping of digital audio to the haptic domain <ref type="bibr">[15]</ref>.
By including the participants in the process, they were able to shape the technology according to their needs but also give them share with them the processes required to reach the end device.</p><p>As part of An Internet of Soft Things project, Kettley et al.
<ref type="bibr">[6]</ref> have developing a new framework to use for participatory design, including relational approaches in psychology, and they use e-textiles in their workshops.
The participants have been learning how to make their own e-textile objects but also have been included as co-researchers, with self-reflection being part of the workshop as much as the making of objects.</p><p>I have explored participatory design in previous research in collaboration with Janet van der Linden <ref type="bibr">[4]</ref>.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
The participants also felt a sense of empowerment in making their own object through touch, as they normally might not be able to engage in craft activities due to visual constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
MY PHD APPROACH</head><p>There is certainly a gap in working with visually impaired users to create their own e-textile objects through participatory design methods, along with a gap in e-textiles research related to touch.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
I also propose to conduct lab studies through which to greater understand people's relationship with different textures and emotions as I believe that this will inform the design process for the workshops when choosing materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES!</head><p>Towards a Wearer-Centred Framework for Animal Biotelemetry</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION</head><p>Since the 1960s, biotelemetry devices such as radio and satellite trackers have been attached to the body of animals for tracking their movements and measuring their vital parameters remotely, allowing the acquisition of ecological, physiological and behavioural information usually inaccessible with other observational methodologies <ref type="bibr">[3]</ref>.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
This raises welfare and ethical concerns <ref type="bibr">[4]</ref> as well as doubts on the validity of acquired experimental data <ref type="bibr">[5]</ref>.
For one example, when studying the foraging behaviour of penguins using transmitters, the tag attached on the body can increase drag, thus reducing the swimming speed and altering the very hunting patterns being investigated <ref type="bibr">[12]</ref>.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
In particular, it aims at developing a framework which can support the implementation of wearer- centred design methodologies and improve wearability aspects of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
BACKGROUND</head><p>Device-induced impacts on individuals have been extensively reported in the literature by animal welfare researchers [e.g.
<ref type="bibr">1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
abnormal grooming in the attempt of removing the foreign body) [review in <ref type="bibr">: 9]</ref>.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Namely, they have pointed out the need to re-design both body-attachment methods, such as collars or harnesses, and tag features, such as weight, shape and colour in a way that better conforms to the wearer <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr">10,</ref><ref type="bibr">12]</ref>.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
For example, in one of their recommendations, researchers discourage the use of the red hue in device components, suggesting that this particular colour can be interpreted as blood by predators or conspecifics <ref type="bibr">[4]</ref>.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
However, many mammal species have di-chromatic vision <ref type="bibr">[6]</ref> and many such predators are driven to prey by scent rather than sight.
For example, wolves have a highly sophisticated olfactory system they use to track prey <ref type="bibr">[2]</ref>, but a scarce ability to detect red objects, perceiving them in shades of grey instead.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
The intent is that of bringing the wearers' perspective into the process of designing biotelemetry devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
DESIGNING FOR WEARABILITY</head><p>In Interaction Design <ref type="bibr">[11]</ref> it has long been established that good interactions are designed 'around' users -their characteristics, those of their activities and those of their environment -systematically informed by established design principles (e.g.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
In this case, it is arguable that paradoxically a good wearer experience amounts to having 'no experience' of the device: in other words, good wearable biotelemetry (for animal wearers who are not users) is one that does not get in the way of the animal's daily experiences, activities or social interactions, one that is not experienced at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.!
Wearer-centred framework</head><p>To this end, this research proposes that the design of wearable devices should be informed by design principles for wearability pertaining to the animal sensory, physical and cognitive experience, namely: sensory imperceptibility, physical unobtrusiveness, and cognitive acceptability.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
electro-receptive animals can sense the electric fields emitted by the tag <ref type="bibr">[8]</ref>) as well as to a wider spectrum of sensitivity (e.g.
birds such as raptors may perceive coloured devices at a much greater distance than people do, thanks to their very acute and pigmented vision <ref type="bibr">[7]</ref>).
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
dense vegetation can impede smooth movements of instrumented animals) <ref type="bibr">[9]</ref>.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
It is proposed that, when designing tags, considering the abovementioned principles in relation to the animal's biology, and consistent with the way in which an animal may experience the device, can help ensure that the devices' features do not generate an impact on the wearer.</p><p>To validate the framework, evaluation studies have been designed, testing off-the-shelf biotelemetry wearables designed for felines.
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
The experimental design has been assessed and approved by the Animal Welfare and Ethical Review Body (AWERB) of the Open University and it also conforms to the University's ACI Research Ethics Protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
CONCLUSIONS</head><p>Biotelemetry has played an important role in the development of behavioural science, and biological sciences more generally.
However, the perspective from which this technology is designed needs an essential and systematic shift.
This research proposes that a wearer-centred framework, and the solutions that can be derived from its application, can help designers in developing devices that impinge less on the animal welfare, thus increasing the data-reliability of biotelemetry interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.!
INTRODUCTION AND TERMINOLOGY</head><p>Scope of this document is to briefly describe the current status of a research project concerning the discovery of Open Educational Resources (OERs).
The term "discovery" indicates search activities whose objectives are not precisely formulated in advance and that require a certain degree of exploration, or "exploratory search" <ref type="bibr">[1]</ref>.
OERs can be defined as any teaching or learning resources that can be freely used as well as repurposed <ref type="bibr">[2]</ref>.
Examples of OERs include interactive exercises, virtual laboratories, assessment tests, lesson plans or even Massive Open Online Courses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
THE PROBLEM</head><p>In the last years, a lot of potentially useful open learning resources have been developed: so many that they have possibly become victims of their own success.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
And yet, the limitations of current applications and their fragmentation are widely recognized as one of the major obstacles to ripe the benefits of the OER movement <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
OBJECTIVE AND RESEARCH QUESTIONS</head><p>There is no intention here to develop yet another OERs search portal or application, many of which are classified and reviewed in the excellent paper <ref type="bibr">[4]</ref>.
Even just standardizing the data (metadata) to describe the educational resources so that they can be found, has proved to be a huge challenge, as demonstrated by large projects such as IMS, LOM, DC <ref type="bibr">[5]</ref>, or LRMI <ref type="bibr">[6]</ref>.
Additionally, while obtaining these metadata proves to be very difficult <ref type="bibr">[7]</ref>, users might not even care about using them <ref type="bibr">[8]</ref>.</p><p>The objective is rather to identify specific discovery oriented functionalities actually required by educators - primarily in the context of Italian high schools, yet to be later generalized in a wider context -and to prototype and evaluate strongly user-oriented innovative re-usable building-blocks <ref type="bibr">[9]</ref> supporting these functionalities, complementing and enhancing existing and future search applications.</p><p>The overall research question is:</p><p>•!
What are the main tasks associated with OERs discovery and how can educators be supported in performing these tasks?</p><p>This is subdivided in the following research sub-questions:</p><p>•!
What tasks teachers need to carry out in relation to OERs discovery, in the framework of the OER life cycle?</p><p>•!
What is the relative importance of tasks and categories identified, and which ones deserve further investigation?</p><p>•!
What are the requirements for tools supporting the identified tasks?</p><p>•!
What is the impact of the prototyped tools on attitude and performances of teachers?</p><p>The research is organized in four main studies described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
DESK-BASED MODEL DRIVEN TASK ANALYSIS</head><p>The first study, a desk-based task analysis, identifies educators' tasks that OERs search/discovery portals should support -analysing the research literature and existing applications, interpreted through a widely accepted behavioural model, the Information Foraging Theory <ref type="bibr">[10]</ref>, modelling the behaviour of modern "informavores" searching for OERs with the same brain-hardwired strategies our ancestors used to hunt for preys.
Result of this study is a comprehensive domain-oriented tasks taxonomy, which is not intended to be immutable, but provides a well-founded framework open to modifications and extensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V.!
TASK ANALYSIS EMPIRICAL EVALUATION</head><p>In the second study, tasks and categories identified in the previous taxonomy have been empirically evaluated, by means of a triangulated qualitative and quantitative analysis of data, collected via survey and structured interviews among actual educators in the target context.
As a first result, the activity has quantified the importance (weights) attributed by educators to the various elements of the taxonomy; weights and taxonomy could be conveniently used in a framework for the evaluation of educational resources search/discovery applications <ref type="bibr">[11]</ref>.</p><p>As an even more important result in this context, the study has established the need to support novel task-oriented expansion/discovery operations, such as discovering additional resources with similar educational alignments as a previously found one, avoiding to force users to think in procedural terms, decomposing the natural task in non- intuitive sequences of lower-level specialization, generalization, and navigation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
PROOF-OF-CONCEPT PROTOTYPE AND INSPECTION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION</head><p>A proof-of-concept prototype has been developed in the third study, based on requirements and insights that have emerged from the previous task analysis empirical evaluation.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
These are identified and ranked via a novel similarity metric, defined as the number of shared alignments to educational standards.</p><p>The prototype has already received very positive feedback, particularly for the possibility to make use of highly valued educational alignments in a transparent way, directly from familiar Google results pages: the Learning Registry <ref type="bibr">[12]</ref>, a major U.S. project jointly financed by the Department of Education and Department of Defense, has manifested the interest to implement the prototype on their infrastructure.
However, a preliminary inspection evaluation has pinpointed some areas for improvements of this first prototype, which will be addressed by a more advanced prototype in the next study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII.
!FINAL PROTOTYPE AND USER TESTING EVALUATION</head><p>The main concern to be addressed in the fourth study is sparsity, that is the intrinsically limited number of educational resources identified within Google results pages, and, within those resources, the limited availability of alignments to educational standards.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
Other possible prototype enhancements include the integration of additional functionalities, and the optimization of its computational complexity.</p><p>A user testing evaluation of this last prototype will be carried out, involving actual educators from the initially targeted context of Italian high schools as well as from a wider context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advances Towards Early Detection of Research Topic</head><p>A trend is usually defined as the general direction in which something is evolving.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
Conversely, I theorise that it is possible to perform very early detection of research trends by identify embryonic topics, <ref type="bibr">which</ref> have not yet been explicitly labelled or identified by a research community, and that is possible to do so by analysing the dynamics of existent topics.
My work is grounded in Kuhn's theory <ref type="bibr">[1]</ref> of the scientific revolution according to which a paradigm shift, also called scientific revolution occurs when a paradigm cannot cope with anomalies, leading to a crisis that will persist until a new outcome redirects research through a new paradigm.
In this abstract, I will discuss the state of the art, present an initial study which supports my hypothesis and outline the future directions of my work.</p><p>The state of the art presents several works regarding the detection of research trends which can be characterised either by the way they define a topic or by the approach they use to discover trends <ref type="bibr">[2]</ref>.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
The Latent Dirichlet Allocation (LDA) <ref type="bibr">[3]</ref> is the most popular technique to extract topics from a corpus using this definition.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
<ref type="bibr">[4]</ref>, in which LDA and citation networks were combined to address the problem of topic evolution.
In some other approaches, keywords are used as proxies for research topics <ref type="bibr">[5]</ref>, but Osborne et al.
<ref type="bibr">[6]</ref> pointed out several drawbacks in using keywords, since they tend to be noisy and carry an unexploited implicit relation between themselves.
Further approaches <ref type="bibr">[7,</ref><ref type="bibr">8]</ref> used taxonomy of topics, which can provide a better characterisation of topics and contain semantic relationships between research areas.</p><p>The approaches for detecting research trends usually rely on statistical techniques to analyse the impact of labels or distributions of words associated to topics <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">8]</ref>.
However, all these approaches focus on already existing topics which are usually already associated with a substantial number of publications.</p><p>Considering the current gap, it is legitimate to ask "How is it possible to detect the early emergence of new research topics?".
Thanks to the availability of very large repositories of scholarly data, nowadays it is possible to address this question.</p><p>To confirm my thesis, I investigated whether the emergence of a new topic is anticipated by specific dynamics among existing ones.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
All these topics were selected within the domain of Computer Science and defined according to the taxonomy produced by <ref type="bibr">Klink [6]</ref>.
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
<ref type="figure" target="#fig_0">Fig.
1</ref> shows the steps of the experiment.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
I then measured the increase in the pace of collaborations in this network with a number of metrics, which involved the analysis of 3-cliques, that are apt to model small collaboration between nodes.</p><p>As reported in my previous work <ref type="bibr">[9]</ref>, the finding shows that the portion of network that is related to a debutant topic exhibits a higher pace of collaboration than the portion of network related to non-debutant topics.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
<ref type="figure">Fig.
2</ref> shows the two distributions of pace of collaboration obtained for the debutant group (blue) and non-debutant groups (orange), in which is possible to see that the distribution of the debutant group is shifted towards positive values while the distribution of the control group is almost centred in zero.</p><p>In conclusion, the results of the preliminary analysis confirm my initial hypothesis, i.e., that it should be possible to anticipate topics emergence by analysing the dynamics between existent ones.
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
within real world systems, perhaps to optimise how those systems can be made to operate.</p><p>The models needed for theses kinds of tasks tend to draw heavily on tabular data.
When John Chambers set out to design the S language (precursor to R, referenced below), he entitled his definitive work, "Programming with Data" <ref type="bibr">[1]</ref>.
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
My scope in this project includes those parts of the tool chain that are concerned with the organising and displaying of (predominantly) tabular data, as opposed to those that do more specialised analysis -say, fitting a generalised linear model or performing combinatorial optimisation.</p><p>At one end of the spectrum, these tools look very much like conventional programming languages, and sometimes are derived from them.
R <ref type="bibr">[2]</ref> is a programming language specifi- cally designed around the needs of programmers working with tabular data, with extensive support for the "data frame" as a container for such data.
Many statistical and other analytical libraries have been written for it.
SciPy <ref type="bibr">[3]</ref> brings similar analytical facilities to the Python language.
SAS <ref type="bibr">[4]</ref> is an older example that remains in widespread current use.</p><p>At the other end of the spectrum are tools that do not require the user to write code (though clearly in an analytical model there must be some means of expressing mathematical relationships).
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
SPSS <ref type="bibr">[5]</ref>, Minitab <ref type="bibr">[6]</ref>) can be driven by a point-and-click interface, usually as an alternative to writing code scripts; the Simul8 package <ref type="bibr">[7]</ref> for discrete event simulation allows the user to set up a simulation entirely graphically.</p><p>Is there a middle ground, that might preserve the capabil- ities for automation and reuse that come from coding, while still providing the accessibility to end users associated with the more interactive tools?
There have been some interesting proposals for "live programming" (see for example <ref type="bibr">[8]</ref>, <ref type="bibr">[9]</ref>), in which the user is able to see intermediate results of code as they write it.
Another approach gaining popularity in the analytical community is "notebook" based analysis <ref type="bibr">[10]</ref>, where the user can freely mix fragments of code, their outputs, textual annotations, and graphics.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
At the same time I am trying to address some of the error- proneness of traditional spreadsheets that has led to numerous problems observed with their use <ref type="bibr">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.
PROPOSED APPROACH</head><p>I am working on proposals not for the actual mechanics of fitting models, but on all the data organisation and documen- tation that goes around the outside.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
I have been building some theory (there is an example in the next section) as to how a basic list-of-lists representation plus some simple constraints can allow a surprisingly wide variety of tabular forms to be modelled, with a minimum of special case rules and special case code.</p><p>One of the sources of errors by spreadsheet users is the lack of explicit demarcation between groups of cells that conceptually represent distinct objects: this carries the twin risks of formula inconsistency between supposedly equivalent cells internal to an object, and incomplete referencing from formulas external to that object.
The <ref type="table" target="#tab_9">Tables feature in newer</ref> versions of Excel helps alleviate this problem, but it is some- what paradoxical that this should be achieved by introducing a new table abstraction quite separate from the one that formed the basis of the spreadsheet all along.
One aspect of the traditional spreadsheet model that I intend to follow is that the same representation is used both for the container and the objects contained; to use an engineering analogy, my data representation is to have a "monocoque" rather than "chassis" construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.
AN EXAMPLE</head><p>Here is a simple example of the kind of approach.
I shall use the small table below, which contains some (fictitious) rainfall data from three weather stations.</p><p>Weather  This representation goes some way to capturing the tabular nature of the data by storing each row in a separate sublist.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
However, if the search engine knows that the user is an MU-FC fan (as she may have clicked on many MU-FC related documents previously), it will move MU-FC related documents to the top of the result list.</p><p>Our approach is to build user profiles based on topics extracted from documents that user clicked on previously.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
".</p><p>A widely used type of user profiles represents the topical interests of the user <ref type="bibr">[3,</ref><ref type="bibr">4]</ref>.
A typical approach is to build user profiles using the main topics discussed in documents that user clicked on <ref type="bibr">[1,</ref><ref type="bibr">8]</ref>.
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Moreover, it requires expensive manual effort to determine the correct categories for each document <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
For instance, the query "Open US" is more likely to be targeting the golf tournament in June, while in September it is the prominent keyword for a tennis tournament.</p><p>To address these problems, we build user profiles dynam- ically using latent topics, which are automatically extracted from the user's relevance documents by an unsupervised topic modelling method (i.e., Latent Dirichlet Allocation -LDA <ref type="bibr">[2]</ref>).
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
To build the user profile dynamically, we propose to use a decay function (e.g., an exponential function) to assign a higher weight to a more recent the user's clicked documents or submitted queries.</p><p>To answer the research question, we adapt our dynamic user profiles to handle two personalisation tasks</p><p>• Personalise search results from the Bing search engine 2 • Personalise query suggestions from the Essex Intranet search engine 3</p><p>For both tasks, we use the query logs of a user from a search engine to build her dynamic profile.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
A log entity consists of an anonymous user-identifier, a submitted query and clicked results along with the user's dwell time.</p><p>For the first task, given an input query, the search system will return a list of the top n documents most relevant to the query.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
To handle the task, we propose three modelling techniques to build user profiles:</p><p>• Enriching profiles dynamically <ref type="bibr">[5]</ref> We first build a user profile using the topics extracted from the user relevant documents (i.e., either a click with a dwell time of at least 30 seconds or the last click in a search session).</p><p>Then, the profile is enriched dynamically with respect to the user's input query by other similar user profiles</p><p>• Building temporal profiles <ref type="bibr">[7]</ref> We build three temporal user profiles the topics extracted from the user relevant documents in different time scales: whole user search history, a search day and a search session</p><p>• Building temporal task-based profiles [6] We build the temporal task-based user profile using the topics extracted from the task-relevant documents For each input query, the proposed user profiles are used to re-rank the document list returned by the Bing search en- gine.
The experimental results showed that the proposed user profiles help improve the search performance significantly.</p><p>For the second task, given an input query, the search system will return a list of the top m suggested queries.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
To handle this task, we first construct two temporal user profiles:</p><p>• A click profile is built using documents that user clicked on.
We build this profile using the same way of building temporal profiles</p><p>• A query profile is build using queries that user submitted to the search system.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
Risk maps would allow the software architect to recommend mitigations such as test driven development or pair programming for the implementation of risky architectural components, and the project manager could increase the risk contingency budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II.!
INTRODUCTION</head><p>According to Goseva-Popstojanova et al <ref type="bibr">[1]</ref>, a risk is "a function of the anticipated occurrence frequency of an undesired event" and that a risk has two parts, probability of failure and severity.
But why should we focus on software architecture for risk assessment?
Lassing et al <ref type="bibr">[2]</ref> asserted that architectural decisions have a major impact on system quality and are expensive to change at a later stage.
Furthermore, Bass et al <ref type="bibr">[3]</ref> claim that the "software architecture inhibits or enables a system's quality attributes".
These views <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> are supported by Bengtsson et al <ref type="bibr">[4]</ref> who state that "tradeoffs between qualities are inevitable and need to be made explicit during architectural design.
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
According to Lindvall et al <ref type="bibr">[5]</ref>, "ripple effects cause problems related to coupling between the pieces that make up a software system".
Since one definition of software architecture is the description of the components and their connectors, a clear link between software architecture and risk is established.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III.!
RELATED WORK</head><p>Surveys reveal many researchers have investigated decision techniques <ref type="bibr">[6,</ref><ref type="bibr">7]</ref> because the ultimate risk is choosing an inappropriate design.
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Other researchers have also considered software architecture evaluation techniques <ref type="bibr">[8]</ref>.
Many techniques focus on evaluating how well an architecture supports specific quality attributes.</p><p>Xiao et al <ref type="bibr">[9]</ref> investigated whether Design Rule Spaces (DRSpaces) could be used to gain architectural insight.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Three conclusions were drawn by Xiao et al <ref type="bibr">[9]</ref>.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
Thirdly, all error prone DRSpaces exhibit multiple structural and evolutionary issues.</p><p>In terms of our research goal, the work of Xiao et al <ref type="bibr">[9]</ref> has a major limitation.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
The results of Xiao et al <ref type="bibr">[9]</ref> suggest that if DRSpaces could be formed from design models prior to implementation, DRSpaces could take on the metaphorical role of the grid squares.</p><p>In our early work <ref type="bibr">[10]</ref> we were able to overcome that limitation by forming DRSpaces from the static analysis of UML class diagrams instead of source code.
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
That is because coupling that stems from the DR, is contained within the DRSpace, and greater coupling has a higher risk of resulting in implementation errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV.!
RESEARCH QUESTIONS</head><p>Whilst the precise research questions and hypotheses remain to be developed, our early work with DRSpaces leads naturally to lots of interesting questions such as: -!
Could architecture partitions be used to isolate, predict and help manage risks?</p><p>-!
What is the best way to partition an architecture to aid technical risk prediction (are some types of partitions more effective for certain kinds of risks)?</p><p>-!
Are risk assessments based on partitions more effective (or are there cases where partitioning hinders risk discovery)?</p><p>V.!
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
<ref type="table" target="#tab_9">Table I</ref> shows the results of partitioning two case studies by DRSpaces and three different flavours of System Use Case (SUC) partitions.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
<ref type="figure" target="#fig_0">Figure 1</ref> shows that in both case studies, DRSpace partitions formed from the design are both isolating and predicting with respect to the risk of implementation errors.
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
Walton Hall Campus Map  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI.!
EARLY RESULTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EEG alpha level envelopes that were used for sonification and corresponding interpolated tracking data.
Left panel-good tracking example (Rho = 0.58), Right panel-bad tracking example (Rho = 0.02).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The mean of the subjective ratings for the six questions of the NASA-TLX: (Exp1 N.17 &amp; Exp2 N. 20).
The error bars show Standard Errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.
2 .Fig.
1 .</head><label>21</label><figDesc>Fig.
2.
Distribution of pace of collaboration for both groups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>weather = [[None, None, None, None], [None, "Weather station name", "Rainfall 2013", "Rainfall 2014"], [None, "Heathrow", 500, 600], [None, "Braemar", 1000, 900], [None, "Cwmystwyth", 1800, 2000]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table of Contents</head><label>of</label><figDesc></figDesc><table>Student Abstracts 

1 
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 

3 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 

5 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 

7 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 

9 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 

11 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 

13 
Deduction in Question Generation 
Pasquale Iero 

15 
Material-Oriented Musical Interaction 
Tom Mudd 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Music for Computer Games To Increase Immersion</head><label></label><figDesc></figDesc><table>Simon Cutajar 

simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 

Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>of Computing and Communications; The Open University</head><label></label><figDesc></figDesc><table>Pasquale Iero 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 

I.
INTRODUCTION 
According to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Tom Mudd tom.mudd@open.ac.uk Computing and Communications / Music Computing Research Lab</head><label></label><figDesc></figDesc><table>Supervisors: Simon Holland, Paul Mulholland 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Dynamic User Modelling for Search Personalisation Thanh Vu thanh.vu@open.ac.uk Computing and Communications Department, The Open University, Milton Keynes, UK</head><label>Dynamic</label><figDesc></figDesc><table>Supervisors names: Dawei Song, Alistair Willis 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Architecture Analysis to Predict Project Risks Using Design Partitions to Predict, Isolate and Manage Risks</head><label>Architecture</label><figDesc></figDesc><table>Andrew Leigh 

andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 

Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 

I.!
MOTIVATION 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>TABLE I .
!
PARTITION RISK PREDICTION AND ISOLATION QUALITIES</head><label>I</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>2</head><label>2</label><figDesc></figDesc><table>Mean Partitions Per Class 

CBO versus Error Proneness (Bug Space %) 

Server Case Study 

Fig.
1.!
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 

01 
01 
13 
31 
05 
12 
41 
40 
02 
39 
10 
11 
07 
26 
06 
36 
18 
19 
14 
37 
16 
20 
21 
33 
17 
34 
22 
38 
03 
04 
29 
23 
24 
28 
25 
27 

</table></figure>

			<note place="foot" n="1"> DLs are, in fact, a family of languages; hence the use of the plural.
The work described here concentrates on core features common to most members of the family.</note>

			<note place="foot" n="2"> http://ontologydesignpatterns.org/wiki/Main_Page 3 A functional property is one for which, for any given subject, there is only one object, e.g.
&apos;hasMother&apos;.</note>

			<note place="foot">Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>

			<note place="foot" n="1"> http://www.dmoz.org/</note>

			<note place="foot" n="2"> http://www.bing.com/ 3 http://search.essex.ac.uk/s/search.html Proc.
of the 2016 CRC PhD Student Conference, Milton Keynes, UK, 23-24 June 2016</note>
		</body>
