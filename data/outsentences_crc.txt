They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).
2 .Fig.
Early assessment of system performance in distributed Real Time Systems 
Mike Giddings 
Managing Conflicting Requirements in Systems of Systems by Adaptation 
Thiago Affonso de Melo Novaes Viana 
Semantic Coupling of Heterogeneous Changes for IR-based Change Request Localization 
Qiuchi Li 
Using insights from psychology and language to improve logic comprehension 
Paul Warren 
The Assessment of Real-Time Electroencephalography Sonification for Neurofeedback 
Tony Steffert 
Towards the Automatic Generation of Dynamic Music for Computer Games To Increase 
Immersion 
Simon Cutajar 
Deduction in Question Generation 
Pasquale Iero 
Material-Oriented Musical Interaction 
Tom Mudd 
simon.cutajar@open.ac.uk 
Department of Computing and Communications, Faculty of Mathematics, Computing and Technology 
Supervisors' names: Robin Laney and Alistair Willis 
Status: Full-Time 
Probation viva: Before 
Starting date: 01/10/2015 
pasquale.iero@open.ac.uk 
Dept.
Names of the Supervisors: Paul Piwek, Allan Third 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/06/2015 
INTRODUCTION 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
Status: Full-Time 
Probation viva: After 
Starting date: 01/10/2013 
andrew.leigh@open.ac.uk 
Computing and Communications Department, The Open University, Milton Keynes, United Kingdom 
Supervisors name/s: Dr. M Wermelinger, Prof. A Zisman 
Status: Part-Time 
Probation viva: Before 
Starting date: 01/10/2015 
MOTIVATION 
CBO versus Error Proneness (Bug Space %) 
Server Case Study 
Fig.
Partition prediction versus isolation qualities Venables Entrance A &amp; C 
Venables Entrance B 
Venables Entrance D 
Venables Entrance E 
Venables Entrance F &amp; Systems 
They frequently overrun cost and time schedules.
The problems are compounded where there are multiple customers and the design responsibilities are split up between different companies based in different countries.
We explore how conventional model driven development can be extended to resolve some of these problems prior to the detailed design of the system components.
An additional model based on loosely coupled functional elements is proposed to represent the behaviour of each system component just after the system requirements are allocated to individual components and the split between hardware and software is established.
In such a model each component is represented as an abstraction of the information contained in the component's contract specification and interface control document coupled to a scheduler.
Functional models can be developed for bespoke and commercial off the shelf components.
Recent surveys have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low at 16% with no expected upturn.
The use of trial and error methods in embedded system development remains at 25%.
Recent surveys by Embedded Systems Design Europe have shown that the adoption of the Unified Modelling Language (UML) in software systems development remains low.
In a survey undertaken by Petre, five categories of usage were identified.
The survey results were: 70% of 50 companies interviewed use no UML at all, 2% at the request by the management or by the client, 22% used and informal selective way, 6% used it for automatic code generation and none at all used it whole heartedly as an organizational commitment to change of culture and practice.
These methods apply to a broad range of software systems and performance requirements.
In particular they cover shared resources, client/servers and event driven systems and mainly focus on business systems.
The proposed methodology, however, enables models to be developed early for systems that comprise individual processing elements, sensors, actuators, displays and controls linked by a bus structure prior to the development of UML models.
These design decisions frequently occur prior to any detailed performance assessment.
Early performance predictions enable performance requirements to be established for individual system elements with a greater confidence than the previous 'fix-it-later' approach.
Additional functional models for each system component of the system mixed with scheduling information for each component may enable the impact of functional changes and real performance to be assessed early in the development process.
Bespoke and Commercial Off The Shelf (COTS) components, bus systems, sensors, actuators, displays and controls can be represented by abstractions based on an iteration schedule.
The first opportunity is to schedule algorithms defined within the definition of each functional element in the functional model in a model animator.
The second opportunity is to animate the object oriented equivalent of the functional chain in the UML models in the MDA PIM for the central processing elements.
These would combine sequence diagrams which represent the functional model functional elements and objects and attributes of objects to represent the notional data flow.
These would be combined with the functional chains for the remaining system elements.
Functional chains have been extracted and investigated in all three cases.
The dynamic errors have been calculated from a functional chain based on the second case study.
An animator, associated with a simple domain model, has been developed which simulates cross system functional chains.
Calculated and animated error predictions have been compared and found to be similar.
The third case study looks at a more complicated functional chain.
A prototype automatic code generator has been developed to automatically generate the animator program code from a database that might be used to record the graphical representation of the functional element model.
30, pp.
In this context, it is important to notice that each independent system should be capable of operate and fulfil useful purposes on its own environment, but, when these independent systems are together and become a SoS they are able to achieve new objectives that could not be achieved by the individual systems.
This means that problems in the SoS environment are harder to handle than in the component system environment.
A common problem that is present into all types of systems is the conflicting requirements.
Also, each component system evolves into a different rhythm.
Therefore, the SoS environment is even more likely to present and be affected by conflicting requirements.
Actually, there are approaches to handle with conflicting requirements, however, no one of them takes into account the differences present in the SoS environment: the independence of each component system, the increased order of complexity, the distributed nature and the dynamic and flexible boundaries.
So, conflicting requirements brings inconsistency, which means that conflicts reflect into problems in the relationship of two or more parts in the system.
However, all these approaches are based on design-time considerations.
But, knowing that each component system was designed at different times and under different circumstances.
Also, that the interaction between them will exist at runtime and that the conflicts will arise by the evolution of them.
Then, these approaches can't be directly applied to manage the conflicting requirements problem in the SoS environment, because this is a runtime problem.
It is important to take into account that: 1.
Each component system is independent and the SoS has a limited control over them; 2.
There are different levels of requirements (the component systems level and the SoS level); 3.
The SoS is a complex distributed system and 4.
The boundaries aren't fixed.
However, this approach is only able to monitor the SoS and is focused just at unexpected events, it doesn't take account how to handle with conflicting requirements and all the implications presents in the inconsistency management.
Therefore, conflicting requirements are harder to manage in the SoS environment.
All these features are very important to a SoS, moreover, they can be considered as basic features to the adaptation process.
As adaptation is a natural mechanism to the SoS handle its problems, it might be a way to support the problem of managing conflicting requirements.
This means that the adaptation process is not just able to receive enough information about the conflicting requirements, but also, the adaptation process is into a perfect position to manage them at runtime.
In the SoS context, these problems are more acute.
This happens because the SoS is an arrangement of complex and independent component systems, so, the differences between each component system, as well as the difference between their goals and stakeholders increase the frequency and impact of conflicting requirements.
Actually, the approaches to manage conflicting requirements at runtime don't take into account all the issues and differences present in the SoS environment: the independence of each component system and the limited control to them by the SoS; The complexity of each component system and the powerful sum of them; The distributed nature of the SoS and its components; The dynamic and flexible boundaries; The many dimensions where the conflicts may happen.
On the other hand, adaptation is a natural issue to the SoS context.
Furthermore, the adaptation process operates at runtime and is able not just to monitor the requirements, but it may also be able to reason about them, identify conflicts and propose actions in the SoS environment, in order to support the management of conflicting requirements.
Thereby, as actually there is no approach able to support the SoS in its tasks to manage the conflicting requirements, and as adaptation is a natural mechanism to the SoS, so, this research aims to the following question: How to support the management of conflicting requirements in the SoS environment by using adaptation?
Many works have been built for concept location.
Among them, a number of researchers are interested in automatically mapping real change requests to source code elements, such as feature descriptions or bug reports.
Others are trying to understand users' concern and represent it with existing source codes in order to enhance program comprehension.
However, both of them are able to tackle one certain form of input, which is not always present in a change request.
Moreover, both approaches are computationally expensive.
Two challenges that are not perfectly solved in the literature are specially tackled: I) Can one give recommendations not only the program elements that should be modified, but also the ones that should be newly created under the IR framework?
To tackle the first challenge, the relevant packages to the CR will be retrieved to suggest that new files should be newly created under the packages.
The semantic representation of each change set will be obtained possibly through program ontology-based approaches.
Then the semantic representation of the input CR is obtained, and matched with the semantically similar change sets.
The task is to map change requests to java class files.
Three of them, namely Elementscoring, Propagation and Aggregation, are investigated onto the CR localization task.
They are all based on unigram language model (UM) to obtain the relevance scores for files, and then compute the package scores based on the package structure and the initial file scores.
The initial study provides a lexical matching framework without involving any historical information.
II) Other structured IR strategies could be investigated on the CR localization problem, supporting localization onto program elements in different level of granularities.
Rather than a mathematical notation, MOS uses a small number of keywords drawn from English, e.g.
and, or, not, only, some.
For example, Rector et al.
It is necessary, e.g.
during the debugging process, to follow a reasoning chain from a set of axioms to a conclusion.
Nguyen et al.
Whilst in many cases the majority of study participants were able to understand the reasoning involved, there were also a significant number of cases where very few participants could follow the reasoning.
A particularly extreme case is the use of only, which in Nguyen et al.
Philosophers of language have studied the subtleties in the use of language, and how natural language differs from formal logic.
Within the cognitive science community there has been considerable research into how "naïve reasoners" (i.e.
those without training in logic) undertake reasoning.
The former suggests that naïve reasoners apply rules similar to those used by trained logicians, e.g.
The latter suggests that mental models are constructed to represent a given situation, e.g.
The distinction between the rule-based and model-based viewpoints can be seen as analogous to that made by logicians between the syntactic and semantic viewpoints.
The debate has been conducted by attempting to use the rival theories to explain mistakes in reasoning made in experimental situations.
The theory proposes that what is important in any reasoning step is the number of variables which need to be simultaneously manipulated.
The patterns were taken from a patterns' portal 2 .
The common constructs were identified from surveys, e.g.
Twenty-one questions were created, each with a set of axioms and a putative inference.
Negated conjunction, i.e.
not (A and B) was frequently not correctly interpreted.
Difficulties reasoning about a functional 3 property.
It was not clear whether the difficulty was due entirely to the high relational complexity of the specific question or whether there is a particular difficulty with functionality.
A participant comment suggested that a difficulty here might be confusion between functionality and inverse functionality.
The study also confirmed the difficulty experienced with negated conjunction, in more complex situations than examined in the first study.
Questions relating to existential and universal quantifiers appeared to exhibit two determinants of performance.
Performance on certain questions suggested the participants were picking up on syntactic clues.
In other cases, errors seemed likely to be caused by the faulty construction of mental models.
Examples of this occurred in some questions involving the universal and existential quantifiers.
An example of the use of the universal quantifier, written in MOS, is hasChild only male.
Not only is this satisfied when there are no non-male children, it is also satisfied when there are no children at all, and this possibility is frequently forgotten.
As an example of the use of the existential quantifier in MOS, hasChild some male includes both the possibilities of having only male children, and of having both male and non-male children.
In situations of high cognitive load, the latter possibility can also be forgotten.
More work is needed to test thoroughly the proposed changes to MOS.
At the same time, these theories may be applicable in other interactions between human and machine.
With Amplitude Modulation (AM) sonification, as the power of the EEG Alpha band increases, the volume of a simple sine wave increases and vice a versa.
The use of pre-recorded EEG data made it possible to control for variability in the signal, and allowed a within-subject study design, where all the participants heard the same sounds.
In order to have an objective measure of how well the real-time changes in activity levels could be continuously perceived, participants were asked to physically track the activity of the sonification with a mouse and slider on the computer screen.
Clearly, having to make this motor response to track the signal instead of just listening to it, will introduce a great deal of lag between the perception of the sound and the movement of the person's hand.
However, this lag applies equally to both sonifications being tested.
Experiment 1 had two aims: Firstly, to identify whether continuous real-time tracking of sonifications by means of a slider and computer mouse could be a practical assessment tool with non-expert users; and, secondly, to assess if the tracking accuracy could distinguish between different sonifications ability to convey the EEG data.
Experiment 2 used the same two sonifications, but in a real- time neurofeedback learning task, (closer to a typical neurofeedback training session), using a simple off-the-shelf commercial EEG device.
A different group of users from Experiment 1 heard a real-time sonification of their own Alpha frequency band brain waves and tried to use these sonifications to control their brain activity.
It was not necessarily anticipated that participants would learn to change their own EEG activity significantly in a single session, but a combination of objective and subjective measures were used to capture any differences in participants' behavior and experience between the two sonifications.
As well as the same task load measure used in Experiment 1, two further questionnaires were used; an assessment of the emotional state of the users and another measured their aesthetic experiences.
Importantly, this allowed Experiment 2 to investigate how perception of a person's own EEG sonification in real-time could differ from the off-line, task in Experiment 1 and whether this simple tracking task could predict outcomes in the real time EEG sonification task.
Therefore, without a quantitative behavioral measure of a person's ability to perceive and track the changes in sonified brain data, the results of the subjective evaluation would lead to the false conclusion that the AM sonification was a better method as it was rated as easier to track.
As in Experiment 1, the NASA- TLX Task Load Index questionnaire showed a preference for AM over FM based sonification.
However, despite no significant change in EEG being observed, the emotion assessment showed that FM-based sonification yielded a stronger reduction in negative emotions.
This could be seen as an indirect corroborative measure of the sonifications effectiveness and yielding a similar pattern of results as experiment 1.
So a participant will "Track" and "Train" one sonification technique each (within-subject design) and there will be two different groups (between-subject design) of the same AM and FM sonification techniques.
However, films are a linear medium that they have a defined start and end, and watching a film again does not change the narrative or the order in which it takes place.
This is not so for most computer games due to their non-linear nature.
A common solution to this problem is for composers to record a finite length of music, and then loop the music in-game.
This is problematic, since once a player realises that the music is being looped, the game's immersive experience is broken.
To address this problem, a core goal of my research is to automatically generate music for computer games that dynamically adapts to the in-game events being experienced by the player.
However, there are very few examples of algorithmic music generation in commercial games.
I have chosen to focus on the use of stochastic methods (particularly Markov chains) to generate music.
These are mathematically very well understood, and much work has been done in their application to music generation.
These viewpoints are normally modeled using Markov chains.
Several systems have been proposed to generate affective music for games, although none to my knowledge have been implemented in commercial games.
If players are within a certain distance from an enemy, the chords to be played are chosen in such a way as to increase tension.
This will be done by making use of a multiple viewpoint system.
The music should also react to events in the game that trigger particular emotions.
The system's output will be evaluated in the context of a computer game, as well as standalone music.
I have identified techniques that have previously been used, and will be focusing on the use of multiple viewpoint systems.
I will also be focusing on the generation of affective music within the context of computer games, in particular, to increase the level of player immersion.
content selection, (3) question type identification, and (4) question construction"[7] The output is not necessarily a question but more generically a request for information.
As input we could have a wealth of possibilities as, for instance, spoken language, diagrams, presentations.
Current systems' focus and mine is on written text.
Therefore a deductive question is based on a conclusion obtained via a deduction from the input text.
We could ask "Is there a phone that is an Android device?"
or "Is there a phone that is not an Android device?"
or "Is it true that no phone is an Android device?".
In a system with existential import, that is if at least one entity exists, these are all deductive questions whose answers can be derived from the original sentence, even if not all of them are affirmative.
The deduction could also be related to the class of the entities involved, i.e.
from the sentence "John plays the flute" we could derive "What musical instrument does John play?"
and from "John plays golf " we could have "What sport does John play?
It's a way of educating about reasoning.
This research has application in the design of virtual agents, developed by the company that supports me, WDS, a Xerox company.
Aggregating information is important in a conversation.
A customer expects an intelligent agent that does not require her to do all the work of finding an answer to her problem.
The conversation must be fluent and the questions asked by the system have to be designed according to economic criteria, i.e.
they don't have to ask the obvious, but they have to ask the relevant.
The process of asking questions is fundamental in acquiring knowledge and forming theories.
This step still requires human creativity and cannot be fully automated.
The article makes a first distinction between questions, seen as interrogative expressions (i.e.
sentences that, in English, terminate with a question mark), and inquiries, that is, as expressions requesting information from the listener (i.e ut- terances, speech acts, etc.).
A deep question such as why, why not, how, what-if, what-if-not taps causal mechanisms.
Its name is MrsQG, in QGSTEC2010, the last Question Generation Shared Task and Evaluation Challenge.
Question type (yes/no, which, what, when, how many, where, why and who) is one of the categories used in the competition.
METHOD Deductive systems all use an abstract representation, even if it is just a parsing tree of the input.
Their syntactic system is represented by the language used to encode this information and the transformations it enables.
The connections asserted in the interpretation of this language represent its semantics.
They describe respectively: facts about concrete situations (e.g.
Harry Potter is a student at Hogwarts), relationships between classes (e.g.
A cheesy pizza is a pizza), relationships between properties (e.g.
Natural Logic has its origins in Aristotle's syllogistic logic.
Modern logic has its roots in the limits of the syllogism.
The reason why this system could be relevant to the research is that it provides logical inference with a minimal form of represent- ation, a Categorial Grammar (CG), while making available quantifiers that are not in FOL, like "most", "few", "half of" and so on.
The idea of formalising semantics is not new.
This approach is typified by a view of the tool as a site for exploration and the development of ideas, as opposed to being a transparent medium through which pre-existing ideas are realised.
Adopting a material-oriented approach highlights the significance of unpredictable elements in creative interactions.
By contrast, the material- oriented perspective foregrounds the importance of exploration and discovery in the interaction with the tool itself.
The artists interviewed by Worth often describe this approach as a way to go beyond their own ideas, to create things that they could not have planned for or anticipated.
This challenges the idea of the user being in complete control of the system, and of the importance of clarity, simplicity and efficiency.
We introduce the term material-oriented interaction (MOI) here to refer to an approach to interaction that views the tool as a source of creativity itself.
Improvising guitarist Derek Bailey describes the instrument as "not just a tool but an ally.
The focus on terms like "exploration" and "natural resources" can be misleading in some ways however as it implies a separation between the apparatus used to find the resources, and the resources themselves.
Although finding new sounds and behaviours is often a stated aim for free improvisation, the exploration is not merely a means to an end, but is a vital part of the process.
An analogy might be made with fishing, where although the stated goal may be to return home with fish, it would miss the point to replace the boat, line, hook, tackle and bait with a machine that merely dispensed fish.
The complex nature of acoustic systems can often make for complex interactions that can be difficult to master, but are nevertheless rich and varied and allow for exploration over an entire lifetime.
Reed instruments such as saxophones and clarinets exemplify this well: very subtle modifications to inputs can lead to vastly different pitches, timbres and volumes.
RIA can therefore be described as a material-oriented approach, but we distinguish it from the term material-oriented interaction as RIA is not concerned with surprises and discoveries that are a direct result of the interaction itself.
In Schön's case, the interaction itself is clear and simple.
The "material" for Schön is the emerging situation rather than the materials of the interaction.
In each case, the interaction is necessarily clear and simple.
The architect would not want a squiggle to emerge where they are trying to draw a straight line.
Following Schön, they support the idea that reflection is not separate from activity, but an integral aspect, and highlight surprise as a mechanism to bring about reflection.
CONCLUSION Creative engagements with tools may encompass a wide range of approaches.
The distinction presented here highlights an approach to interaction that embraces surprise and complexity.
As discussed by Philippe et al.
The evaluation of a fabric is explored through how we feel when holding it in our hands.
The way that we might explore a surface, depending on what knowledge we want to acquire about it, has been defined by Lederman et al.
An example is the use of object symbols (also known as objects of reference), as discussed by McLinden et al.
These are often used with children who are deafblind, or have multiple disabilities with a visual impairment, to promote communication skills.
Object symbols also have associations attached to them.
Bloom [1:8] describes them as 'three dimensional iconic symbols that can be used to represent real objects or events.'
An example of an object of reference is a wooden spoon that might be passed to a user for them to touch, to signify that it is time to cook.
I am interested in how touch can play a role in my own research with objects and hands-on making.
My focus is the use of electronic textiles (e-textiles) through touch, a sense that is less explored in this field, with the visual taking priority.
As they are conductive, and come in the form of yarns, thread and fabrics, e-textile materials can be spun, sewn or woven into sensors that can be touched to trigger an action.
Perner-Wilson et al.
These items may be beautiful but there are audiences and users who cannot engage with them due to a lack of sight.
There is a body of research which does engage with touch using e-textiles, with this often being more therapy related projects.
Examples of this is the work by Vaucelle et al.
Rode et al.
They found that, as well as computational thinking, additional skills are needed such as creativity and understanding materials.
Children and teenagers are often the focus group for maker workshops that use e-textiles and creative technology.
Groups that are missed are often those with a disability or impairment, particularly blind and visually impaired users who might rely more on touch as a sense.
How might these users be included in this process so that they too can make objects using these technologies?
that Participatory design approaches '…seek to include future users in most parts of the design process, even as co-designers.
Including the users as co-designers or co- researchers is a methodology that is explored increasingly when working with users who have an impairment or disability.
Metatla et al.
We facilitated e- textile weaving workshops during a pilot study with blind and visually impaired users in which they could weave their own touch-based e-textile interface to use for sound interaction.
The benefits of this approach is not only the creation of an object which the user has co-designed with the researcher, but also the valuable discussions which come out of the process, with potential usability and design ideas for future applications.
These discussions also often refer to biographical information about the participant or associations they form whilst making their object.
My proposed research question is 'How can e- textiles be used as a vehicle for capturing memories through touch, particularly through participatory design processes?'
From running the pilot e-textile weaving workshops I found that participatory design methods were successful and would like to use these in my research going forward.
Although the use of biotelemetry has revolutionized data-gathering practices in animal biology and ecology, the presence of a device on the animal body may generate side effects to wearers.
In order to decrease device-induced impacts, this research aims at bringing the perspective of animal stakeholders into the design process of biotelemetry technologies, proposing the application of Interaction Design approaches for the design of body-attached devices used on animals.
Concisely, such impacts can be physically manifested, such as those occurring on the site of attachment (e.g.
fur abrasion, limb swelling, or wounds), or less obviously perceivable, such as alterations of physiological parameters (e.g.
variations in the metabolic activity and body temperature).
Changes in the normal behaviour can be apparently unrelated with the presence of the device (e.g.
decrease in foraging behaviour) or clearly derived from it (e.g.
To reduce such negative effects, animal welfare scientists have proposed guidelines and recommendations for improving the designs of biotelemetry devices.
Arguably, whereby negative effects induced by a device are minimized or removed, the quality of collected data can be improved as well as the welfare of the animal being monitored.
However, although in principle such guidelines aim to bring the perspective of the animal to the attention of researchers and designers, when it comes to application details they often lack the very perspective they advocate.
Indeed, this may be the case if said predators or conspecifics are able to see colours in the same way that humans do, and more importantly, if they use sight as the guiding sense towards blood, and colour as blood's characterising feature.
Although a red harness or tag encase could generate an impact (for example, by disrupting the camouflage of an animal), the argument is that design recommendations should be informed by criteria that systematically extend beyond the human perspective (e.g.
associating the colour red with blood and colour as a salient marker of blood).
However, to date there is no design framework that can help researchers and designers to systematically account for and reconcile the often diverging perspectives of the animal wearer and of the human user, and the design requirements that derive from both.
For example, ecologists often use coloured tags for marking the animals they are studying because they need to easily identify individuals during field observations; but this can be detrimental for the animals if they become easily detectable to ill-intentioned humans, potential predators or prey.
To address this gap, the development of a wearer-centred framework has been started with this research.
perceivability, affordance).
Although these include physical interactions resulting from the use of, or contact with, wearable technologies, a design framework to support the design of wearer-centred devices is still lacking.
Developing such a framework is the aim of this research, particularly with reference to the design of biotelemetry for animals, in order to improve their wearer experience and therefore reduce the negative aspects related to the device presence.
Providing good user experience is a main goal of user-centred technology, with the fundamental assumption that the technology the users interact with is directly relevant to them and their activities.
But what is the equivalent of a 'good experience' when the interaction that comes from a technology is essentially physical and sensory rather than cognitive and active?
This is arguably the position in which animal wearers of biotelemetry find themselves in when they physically interact with technologies that do not serve their own purposes but those of someone else.
Sensory perceptibility refers to a wider range of senses in comparison with those of humans (e.g.
Physical obtrusiveness is linked with locomotive abilities (e.g.
swimming or flying movements can be limited by a tag attached in an improper location) and environmental features (e.g.
Cognitive unacceptability refers to the psychological condition of those animals that, being aware of the device, do not accept its presence, which can lead to the development of atypical behaviour such as stereotypes (detrimental compulsions that may arise when a wearer cannot express its natural behaviour because of the tag).
The purpose here is that of observing which tag features, and to what extent, they are liable to produce side effects on the wearer (for example, fur abrasion on the site of attachment, increment in grooming, or movement intrusiveness).
The findings will serve as an initial validation of the framework.
As a complementary form of validation, the framework will be then applied to inform the design of prototype tags, which will be compared against off- the-shelf devices.
The comparison between impacts produced by commercial products and the prototype will provide insights as to the validity of the framework.
At the moment a first study testing an off-the-shelf GPS tracker has been carried out on two domestic cats.
Cats have been chosen as the initial model species, with a view to extend future works to other species.
However, the perspective from which this technology is designed needs an essential and systematic shift.
There are currently hundreds repositories of educational resources, federations of repositories, federations of federations of repositories -each one with their proprietary, largely incompatible, search application.
A large amount of money has been spent on projects aiming at developing solutions to the problem of OERs discoverability, including many European research projects.
This prototype identifies educational resources, directly in Google results pages (the platform used by most educators for finding OERs), where it injects custom rich-snippets containing descriptive metadata and expansion/discovery functionalities, allowing users to expand their search to similar resources.
A number of possible solutions to address this challenge, to be further analysed and experimented with in the remaining project activities, have been determined.
These include automatic replication of the Google search in specialized portals, modification of Google ranking to include at least a minimum number of educational resources in the first displayed page, integration of expansion functionalities on top of a traditional search application, restriction of Google results to specialized portals via a Custom Search Engine.
It is often used to describe the popularity of items, such as brands, words, and technologies.
In order to detect trends, the relevant items should usually be already recognized and often somewhat popular.
For this reason, current methods for detecting trends of research topics usually focus on identifying terms associated with a substantial number of documents, which usually took some years to be produced.
A common approach to identifying research topic is the probabilistic topic model, in which a topic is characterised as a multinomial distribution over words.
Since its introduction, LDA has been extended and adapted in many other applications as it can be seen in He et al.
To do so, I introduced a novel method for assessing the increase in the pace of collaborations in topic networks and tested it on more than 2000 co-occurring topics and 3 million research publications from the Rexplore system: http://technologies.kmi.open.ac.uk/rexplore.
In particular, I randomly selected 50 topics that emerged in the decade 2000- 10 for my treatment group (debutant group) and 50 well established topics as a control group (non-debutant group).
The experiment itself consisted in two phases.
In the first phase, I selected and extracted portions of the collaboration network that was related to the topics in the two groups in the few years prior the year of their debut (the topics in the control group were associated to random years).
Afterwards, I analysed the overall pace of collaboration for each network associated to these test topics.
The selection phase is based upon the assumption that an emerging topic will tend to collaborate with its procreators.
Therefore, these topics could be analysed in the previous years to confirm my theory.
Hence, for each topic in the two groups, I selected their n (20, 40, 60) most co-occurring ones and then extracted the portion of their collaboration network containing these topics in the five years prior to the year of analysis.
A collaboration network is a fully weighted graph in which nodes are represented by topics and their weight represent the number of papers in which they appear, while links between nodes and their weights represent the amount of papers they co-occur together.
At the end of this stage, each tested topic was associated with five collaboration networks, representing the behaviour of its predecessors in the five previous year.
In particular, by using an approach based on the linear regression of the collaboration pace associated to each year it is possible to effectively discriminate the two groups of topics.
In addition, by analysing the collaboration networks containing 20, 40 and 60 most co- occurring topics, I found out that increasing the size of collaboration networks, the approach provides better results.
A reason for this can be that increasing the number of topics in the collaboration network increases the chances to select its procreators.
I performed the Student's t-test over the distributions associated with the two groups, obtaining p-values less than 0.0001, which allows me to reject the null hypothesis H0: "The differences in the pace of collaboration between the debutant topics and topics in the control group result purely from chance".
I plan to further develop my approach in two main directions.
First, I am currently working on a method for the automatic detection of embryonic topics that analyses the topic network and identifies sub-graphs where topics exhibit the discussed dynamics.
A second direction of work focuses on improving the current approach by integrating a number of additional dynamics involving other research entities, such as authors and venues.
The aim is to produce a robust approach that could be used by researchers and companies alike for gaining a better understanding of where research is heading.
Analysts working in the fields of statistics, data science, and operational research frequently engage in end user de- velopment: in order to carry out their analyses, they have to perform activities that resemble programming, and may actually involve writing code, but which are not full scale software development.
The kinds of tasks these users undertake include constructing database queries; applying the techniques of inferential statistics to deduce relationships; visualisation of data; and the modelling of flows (of people, goods, etc.)
He might equally have chosen the title, "Programming with Tables".
A wide variety of tools have been developed for the purpose of working with tabular data in the context of these tasks.
Many statistical and other analytical libraries have been written for it.
The most ubiquitous example is the spreadsheet, but some more specialised tools would also fit into this category.
For example, many statistical packages (e.g.
My work leans towards the latter approach but seeks to make it a little more data-centric, regaining some of the flexibility of layout and directness of interaction that have made the spreadsheet such an accessible tool.
This year I am working mainly on data representation, and in future years will expand on this to deal with the more "programming" type aspects of how the user orchestrates the processing of the data.
My current objective is to develop a suitable representation for structured collections of partly tabular, partly freeform data - for "messy tables".
My starting point is the observation that the modern scripting languages (e.g.
Python, Ruby, Javascript) have adopted lists and hash tables as their staple container types.
These very flexible building blocks allow a wide range of objects to be modelled.
So my basic approach is to start from a visual analogue of these types.
Of itself, the representation does not express the constraint that all rows are required to have the same number of elements, one for each column.
This additional constraint could be captured in various ways -we could employ a specialised Table class, or use some kind of tagged notation as in HTML.
I am exploring an alternative, where items that are supposed to be kept aligned are defined by the nature of the list structure, using the head of each list as a kind of template.
The table above would become: The None symbol which is Python's null value acts as a kind of placeholder, but is not wasted space: in an expanded version of this representation, the nulls above will become row and column properties.
The sequence of four nulls at the beginning announces that subsequent elements in the weather list are also to be lists of four elements.
The initial null at the start of the other sublists releases their elements from further constraints.
The potential value in this approach is that it can provide a flexible structure good for capturing "messy" tables and their layout.
By extending this very simple notion -that the head element of each list specifies some structural constraints on remaining elements -it is possible to represent much more complicated structures such as higher dimensional tables, hierarchies of subdivisions within tables, and mixed collections of tables and freeform lists.
There is no need to specify different types or tags for all these things -they simply emerge naturally as consequences of the one underpinning rule.
This can potentially offer the user greater flexibility and simplicity of use.
My research focuses on building user profiles dynamically to improve the performance of search personalisation.
Unlike classical information retrieval (IR) systems, such as Altavista and AOL, personalised search engines utilise the personal data of each user to tailor search results to that user.
Such personal data can be used to construct a user profile, which is crucial to effective personalisation.
For instance, if a user submits an input query "MU" to a search engine, she possibly needs information about the Manchester United Football Club (MU- FC) or about the Musician's Union.
The user's search interests may change from time to time, and are reflected changing in their interactions with the system (e.g., clicks on documents).
The user profile should, therefore, be built dynamically so that the profile constantly adapts to represent the user's search interests.
This leads to the research question "Can a dynamic user profile be used to improve the performance of search personalisation?".
To answer the research question, we need to address the following question "How can we build the user profile in such a dynamic way that it quickly adapts to represent the user's search interests?
The topics of a document can be obtained from a human-generated online ontology, such as the Open Directory Project (ODP) 1 .
However, this approach suffers from a limitation that many documents do not appear in the online categorisation scheme.
Apart from these problems, previous studies largely ignored the dynamic nature of the user's interests; i.e., with the change of time, the search intent and user interests may also change.
From a document collection as the input, the LDA algorithm output will be n topics.
Each topic is described as a distribution over a fixed vocabulary; and each word has a different propor- tion on the topic.
For example, a football-related topic would give high proportions to words like "Score" and "Goal" whilst the OS-related topic gives high proportions to "Windows" and "Linux".
Moreover, each document in the collection is also described as a distribution over topics, in which each topic has a different proportion on the document.
The proportion of each topic indicates how relevant the document is to the topic.
For example, a document about a football match would give a high proportion to the topic about "football" and a low proportion to the "OS" topic.
By doing that, we use latent topics extracted from documents that user clicked on together with the interaction time.
For each query, our goal is to adapt the dynamic user profile to re-rank the search result list of the query.
After re- ranking, we will achieve a higher search performance if more relevant documents are promoted to higher ranks.
For each query, our goal is to re-rank the suggestion list of the query using the dynamic user profiles.
We then use both profiles to re-rank the suggestion list returned by the Essex Intranet search engine.
The experimental results showed that these user profiles help significantly improve the quality of query suggestions.
Using both the click profile and query profile achieved the highest performance indicating that the personalised query suggestion for Intranet search should take into account both click and query information.
We are motivated to help software development projects be more successful in terms of budget and schedule as many fail when measured in these terms.
Risk assessments and risk management are good project management practice.
The ultimate goal of our research is to use architecture analysis to predict project risks so that they can be avoided and managed by creating risk maps of software architectures.
The mountaineer is safer if they know when to use crampons and ropes.
But why should we focus on software architecture for risk assessment?
This is especially important because architectural designs are generally very hard to change at a later stage".
Why should there be such a relationship between technical decisions and effort to change (cost)?
The non-binary relationship between design alternatives and stakeholder satisfaction suggests that a risk based approach to evaluation is necessary.
Each DRSpace is based on a design rule which are the key interfaces that partition an architecture into independent modules.
The authors explain that each DRSpace is a graph whose vertices are the related classes the DRSpace is composed of and the edges are the relationships between those related classes.
Firstly, that if a leading file of a DRSpace is error-prone, a large proportion of the other DRSpace files are likely to be error prone.
Secondly, that most error prone files will be found in just a few DRSpaces.
It is dependent on static analysis of source code.
But returning to the mountaineering metaphor, an interesting question to consider is what would be the grid squares of a software architecture risk map in order for it to be annotated with risks?
Furthermore, we observed a strong and significant positive correlation between coupling metrics computed from the design and implementation error-proneness.
Whilst the effect of coupling is well understood, our contribution was to demonstrate that DRSpaces are isolating risk containers (partitions).
RESEARCH METHODS Our research methods are in the early stages of consideration but we see industrial cases studies as providing rich evidence with which to test our hypotheses and answer our research questions.
We have already successfully exported data from a commercial software development company but access to further data and a wider range of case studies is seen as our biggest research project risk.
The general methods we expect to use include: calculating partition level design metrics and comparing to implementation error/change proneness, comparing retrospective partition based risk assessments to implementation error/change proneness, comparing both retrospective partition based risk assessments, and non- partition based risk assessments (control) with real project issues encountered.
The next section presents some further early results from our first industrial case studies.
Partitions have been compared for Spearman's Rank correlation between design Coupling Between Objects (CBO) and implementation error-proneness (predicting) and mean partitions per class (isolating).
The results also show that for this particular risk, DRSpaces are better risk partitions than SUCs because the more comprehensive and predicting we make the SUC partitions the less isolating they become.
This is to be expected because DRSpaces are reusable modules, whereas SUCs are system functions fulfilled by re-using such modules where possible, in good software engineering practice.
Another compelling observation is that the correlation between design CBO and error proneness is stronger for DRSpace partitions than it is for individual classes (control).

